{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Himself65 (Himself65)',\n",
       " 'Dessalines (dessalines)',\n",
       " 'Harrison Chase (hwchase17)',\n",
       " 'Nicolas Patry (Narsil)',\n",
       " 'Jeff Dickey (jdxcode)',\n",
       " 'Mattias Wadman (wader)',\n",
       " 'dgtlmoon (Jerry Liu)',\n",
       " 'jerryjliu (Ha Thach)',\n",
       " 'hathach (Nuno Campos)',\n",
       " 'nfcampos (Chris Banes)',\n",
       " 'chrisbanes (Marcus Olsson)',\n",
       " 'marcusolsson (Zoltan Kochan)',\n",
       " 'zkochan (MichaIng)',\n",
       " 'Alex Kladov (matklad)',\n",
       " 'Alessandro Ros (aler9)',\n",
       " 'Toby Mao (tobymao)',\n",
       " 'Alexander Borsuk (biodranik)',\n",
       " 'Fons van der Plas (fonsp)',\n",
       " 'syuilo (syuilo)',\n",
       " 'atomiks (josegonzalez)',\n",
       " 'Azure SDK Bot (azure-sdk)',\n",
       " 'Didier Peran Ganthier (didierganthier)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "all_names = [tag.string for tag in parsed_html.find_all(\"a\",{\"data-hydro-click\": re.compile(\"TRENDING_DEVELOPERS_PAGE\")})]\n",
    "all_names_2 = [i.strip() for i in [str(x) for x in all_names] if i != \"None\"]\n",
    "[i + \" \" + \"(\"+j+\")\" for i,j in zip(all_names_2[0::2],all_names_2[1::2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelscope (modelscope)',\n",
       " 'Stability-AI (stablediffusion)',\n",
       " 'GaiZhenbiao (ChuanhuChatGPT)',\n",
       " 'home-assistant (core)',\n",
       " 'AUTOMATIC1111 (stable-diffusion-webui)',\n",
       " '34j (so-vits-svc-fork)',\n",
       " 'acheong08 (EdgeGPT)',\n",
       " 'bregman-arie (devops-exercises)',\n",
       " 'karpathy (nanoGPT)',\n",
       " 'TencentARC (GFPGAN)',\n",
       " 'comfyanonymous (ComfyUI)',\n",
       " 'hwchase17 (chat-langchain)',\n",
       " 'lucidrains (PaLM-rlhf-pytorch)',\n",
       " 'Akegarasu (ChatGLM-webui)',\n",
       " 'neonbjb (tortoise-tts)',\n",
       " 'zhayujie (chatgpt-on-wechat)',\n",
       " 'acheong08 (ChatGPT)',\n",
       " 'Zero6992 (chatGPT-discord-bot)',\n",
       " 'pytube (pytube)',\n",
       " 'pointnetwork (point-alpaca)',\n",
       " 'iperov (DeepFaceLab)',\n",
       " 'hwchase17 (langchain)',\n",
       " 'crowsonkb (k-diffusion)',\n",
       " 'karfly (chatgpt_telegram_bot)',\n",
       " 'openai (whisper)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "parsed_html.find_all('a',{'class':'href'})\n",
    "\n",
    "trend_rep = [tag.attrs for tag in parsed_html.find_all(\"a\",{\"data-hydro-click\": re.compile(\"TRENDING_REPOSITORIES_PAGE\")})]\n",
    "\n",
    "trend_rep2 = [i.split(\"/\") for i in [i[\"href\"] for i in trend_rep] if i.count(\"/\") == 2]\n",
    "\n",
    "for i in trend_rep2:\n",
    "    i.remove(\"\")\n",
    "\n",
    "trend_repf = []\n",
    "\n",
    "for i in [i[0] + \" \" + \"(\"+i[1]+\")\" for i in trend_rep2]:\n",
    "    if not \"apps\" in i:\n",
    "        trend_repf.append(i)\n",
    "\n",
    "trend_repf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/38px-Wikisource-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/34px-Wikiquote-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/14px-Commons-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/16px-Wikiquote-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/18px-Wikisource-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/21px-Wikidata-logo.svg.png\n",
      "\n",
      "https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "images = parsed_html.find_all('img', {'src':True})\n",
    "for image in images:\n",
    "    if \"upload.wikimedia\" in image['src']:\n",
    "        print('https:'+image['src']+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bodyContent\n",
      "/wiki/Main_Page\n",
      "/wiki/Special:Search\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Python\n",
      "/w/index.php?title=Special:UserLogin&returnto=Python\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Python\n",
      "/w/index.php?title=Special:UserLogin&returnto=Python\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Special:MyContributions\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "/wiki/Wikipedia:About\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "/wiki/Help:Contents\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "/wiki/Wikipedia:File_upload_wizard\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Python&oldid=1143380376\n",
      "/w/index.php?title=Python&action=info\n",
      "/w/index.php?title=Special:CiteThisPage&page=Python&id=1143380376&wpFormIdentifier=titleform\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452\n",
      "/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen\n",
      "/w/index.php?title=Python&printable=yes\n",
      "https://commons.wikimedia.org/wiki/Category:Python\n",
      "#p-lang-btn\n",
      "#\n",
      "#Snakes\n",
      "#Computing\n",
      "#People\n",
      "#Roller_coasters\n",
      "#Vehicles\n",
      "#Weaponry\n",
      "#Other_uses\n",
      "#See_also\n",
      "https://af.wikipedia.org/wiki/Python\n",
      "https://als.wikipedia.org/wiki/Python\n",
      "https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)\n",
      "https://az.wikipedia.org/wiki/Python_(d%C9%99qiql%C9%99%C5%9Fdirm%C9%99)\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)\n",
      "https://be.wikipedia.org/wiki/Python\n",
      "https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)\n",
      "https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)\n",
      "https://da.wikipedia.org/wiki/Python\n",
      "https://de.wikipedia.org/wiki/Python\n",
      "https://eo.wikipedia.org/wiki/Pitono_(apartigilo)\n",
      "https://eu.wikipedia.org/wiki/Python_(argipena)\n",
      "https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86\n",
      "https://fr.wikipedia.org/wiki/Python\n",
      "https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0\n",
      "https://hr.wikipedia.org/wiki/Python_(razdvojba)\n",
      "https://io.wikipedia.org/wiki/Pitono\n",
      "https://id.wikipedia.org/wiki/Python\n",
      "https://ia.wikipedia.org/wiki/Python_(disambiguation)\n",
      "https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)\n",
      "https://it.wikipedia.org/wiki/Python_(disambigua)\n",
      "https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F\n",
      "https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)\n",
      "https://kg.wikipedia.org/wiki/Mboma_(nyoka)\n",
      "https://la.wikipedia.org/wiki/Python_(discretiva)\n",
      "https://lb.wikipedia.org/wiki/Python\n",
      "https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)\n",
      "https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)\n",
      "https://nl.wikipedia.org/wiki/Python\n",
      "https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3\n",
      "https://no.wikipedia.org/wiki/Pyton\n",
      "https://pl.wikipedia.org/wiki/Pyton\n",
      "https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)\n",
      "https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)\n",
      "https://sk.wikipedia.org/wiki/Python\n",
      "https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)\n",
      "https://sh.wikipedia.org/wiki/Python\n",
      "https://fi.wikipedia.org/wiki/Python\n",
      "https://sv.wikipedia.org/wiki/Pyton\n",
      "https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)\n",
      "https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD\n",
      "https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86\n",
      "https://vi.wikipedia.org/wiki/Python\n",
      "https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/w/index.php?title=Python&action=edit\n",
      "/w/index.php?title=Python&action=history\n",
      "/wiki/Python\n",
      "/w/index.php?title=Python&action=edit\n",
      "/w/index.php?title=Python&action=history\n",
      "https://en.wiktionary.org/wiki/Python\n",
      "https://en.wiktionary.org/wiki/python\n",
      "/w/index.php?title=Python&action=edit&section=1\n",
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/wiki/Python_(mythology)\n",
      "/w/index.php?title=Python&action=edit&section=2\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/w/index.php?title=Python&action=edit&section=3\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/wiki/Python_Anghelo\n",
      "/w/index.php?title=Python&action=edit&section=4\n",
      "/wiki/Python_(Efteling)\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/w/index.php?title=Python&action=edit&section=5\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/w/index.php?title=Python&action=edit&section=6\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/wiki/Colt_Python\n",
      "/w/index.php?title=Python&action=edit&section=7\n",
      "/wiki/Python_(codename)\n",
      "/wiki/Python_(film)\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/wiki/Timon_of_Phlius\n",
      "/w/index.php?title=Python&action=edit&section=8\n",
      "/wiki/Pyton\n",
      "/wiki/Pithon\n",
      "/wiki/File:Disambig_gray.svg\n",
      "/wiki/Help:Disambiguation\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Python&oldid=1143380376\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Human_name_disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_given-name-holder_lists\n",
      "/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "//creativecommons.org/licenses/by-sa/3.0/\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile\n",
      "https://developer.wikimedia.org\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "for link in parsed_html.findAll(\"a\"):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "\n",
    "\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 5 - Government Organization and Employees',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 8 - Aliens and Nationality',\n",
       " 'Title 11 - Bankruptcy',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 18 - Crimes and Criminal Procedure',\n",
       " 'Title 22 - Foreign Relations and Intercourse',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 28 - Judiciary and Judicial Procedure',\n",
       " 'Title 34 - Crime Control and Law Enforcement',\n",
       " 'Title 36 - Patriotic and National Observances, Ceremonies, and Organizations',\n",
       " \"Title 38 - Veterans' Benefits\",\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 50 - War and National Defense',\n",
       " 'Title 54 - National Park Service and Related Programs']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "changed_titles = [str(tag) for tag in parsed_html.find_all(\"div\",{\"class\": \"usctitlechanged\"})]\n",
    "changed_titles_list = [j.split(\" <\") for i in [i.split(\"          \") for i in changed_titles] for j in i]\n",
    "\n",
    "[i[0].strip() for i in changed_titles_list[1::2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALEXIS FLORES',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'RUJA IGNATOVA',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'MICHAEL JAMES PRATT',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "top_wanted = [tag.attrs for tag in parsed_html.find_all(\"img\",{\"alt\": True})]\n",
    "\n",
    "[i[\"alt\"] for i in top_wanted if \"topten\" in i[\"src\"]]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:28:38.3</td>\n",
       "      <td>42.43 N</td>\n",
       "      <td>143.27 E</td>\n",
       "      <td>HOKKAIDO, JAPAN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:27:23.9</td>\n",
       "      <td>38.34 N</td>\n",
       "      <td>27.17 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:24:29.3</td>\n",
       "      <td>38.03 N</td>\n",
       "      <td>36.67 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:19:30.0</td>\n",
       "      <td>9.66 N</td>\n",
       "      <td>84.81 W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:13:53.0</td>\n",
       "      <td>38.50 N</td>\n",
       "      <td>38.02 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:13:28.0</td>\n",
       "      <td>7.36 N</td>\n",
       "      <td>123.80 E</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:10:38.0</td>\n",
       "      <td>37.32 N</td>\n",
       "      <td>36.67 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>21:01:12.8</td>\n",
       "      <td>38.94 N</td>\n",
       "      <td>21.23 E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:59:17.0</td>\n",
       "      <td>11.95 N</td>\n",
       "      <td>86.34 W</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:50:58.6</td>\n",
       "      <td>38.92 N</td>\n",
       "      <td>21.21 E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:50:46.5</td>\n",
       "      <td>38.23 N</td>\n",
       "      <td>38.57 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:42:44.8</td>\n",
       "      <td>38.03 N</td>\n",
       "      <td>37.83 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:38:52.0</td>\n",
       "      <td>0.81 N</td>\n",
       "      <td>126.23 E</td>\n",
       "      <td>MOLUCCA SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:30:59.9</td>\n",
       "      <td>38.28 N</td>\n",
       "      <td>37.79 E</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:29:54.0</td>\n",
       "      <td>1.99 N</td>\n",
       "      <td>98.97 E</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:28:15.3</td>\n",
       "      <td>38.17 N</td>\n",
       "      <td>38.68 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:15:04.1</td>\n",
       "      <td>42.78 N</td>\n",
       "      <td>9.01 W</td>\n",
       "      <td>SPAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:05:14.6</td>\n",
       "      <td>43.40 N</td>\n",
       "      <td>18.38 E</td>\n",
       "      <td>BOSNIA AND HERZEGOVINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:04:20.8</td>\n",
       "      <td>38.36 N</td>\n",
       "      <td>27.22 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:04:00.0</td>\n",
       "      <td>13.01 N</td>\n",
       "      <td>89.02 W</td>\n",
       "      <td>OFFSHORE EL SALVADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>20:03:52.0</td>\n",
       "      <td>5.63 S</td>\n",
       "      <td>103.16 E</td>\n",
       "      <td>SOUTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Time Latitude Longitude                       Region\n",
       "0   2023-03-20  21:28:38.3  42.43 N  143.27 E       HOKKAIDO, JAPAN REGION\n",
       "1   2023-03-20  21:27:23.9  38.34 N   27.17 E               WESTERN TURKEY\n",
       "2   2023-03-20  21:24:29.3  38.03 N   36.67 E               CENTRAL TURKEY\n",
       "3   2023-03-20  21:19:30.0   9.66 N   84.81 W                   COSTA RICA\n",
       "4   2023-03-20  21:13:53.0  38.50 N   38.02 E               EASTERN TURKEY\n",
       "5   2023-03-20  21:13:28.0   7.36 N  123.80 E        MINDANAO, PHILIPPINES\n",
       "6   2023-03-20  21:10:38.0  37.32 N   36.67 E               CENTRAL TURKEY\n",
       "7   2023-03-20  21:01:12.8  38.94 N   21.23 E                       GREECE\n",
       "8   2023-03-20  20:59:17.0  11.95 N   86.34 W      NEAR COAST OF NICARAGUA\n",
       "9   2023-03-20  20:50:58.6  38.92 N   21.21 E                       GREECE\n",
       "10  2023-03-20  20:50:46.5  38.23 N   38.57 E               EASTERN TURKEY\n",
       "11  2023-03-20  20:42:44.8  38.03 N   37.83 E               CENTRAL TURKEY\n",
       "12  2023-03-20  20:38:52.0   0.81 N  126.23 E                  MOLUCCA SEA\n",
       "13  2023-03-20  20:30:59.9  38.28 N   37.79 E               CENTRAL TURKEY\n",
       "14  2023-03-20  20:29:54.0   1.99 N   98.97 E  NORTHERN SUMATRA, INDONESIA\n",
       "15  2023-03-20  20:28:15.3  38.17 N   38.68 E               EASTERN TURKEY\n",
       "16  2023-03-20  20:15:04.1  42.78 N    9.01 W                        SPAIN\n",
       "17  2023-03-20  20:05:14.6  43.40 N   18.38 E       BOSNIA AND HERZEGOVINA\n",
       "18  2023-03-20  20:04:20.8  38.36 N   27.22 E               WESTERN TURKEY\n",
       "19  2023-03-20  20:04:00.0  13.01 N   89.02 W         OFFSHORE EL SALVADOR\n",
       "20  2023-03-20  20:03:52.0   5.63 S  103.16 E  SOUTHERN SUMATRA, INDONESIA"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# Date and Time\n",
    "\n",
    "date_time = [str(tag.string) for tag in parsed_html.find_all(\"a\",{\"href\": True})]\n",
    "date_time = [i.replace(\"\\xa0\",\" \") for i in date_time if \"2023\" in i]\n",
    "date_time =[i.split(\"   \") for i in date_time]\n",
    "\n",
    "date = [i[0] for i in date_time]\n",
    "time = [i[1] for i in date_time]\n",
    "\n",
    "# Latitude and Longitude\n",
    "\n",
    "latitude_longitude1 = [str(tag.string) for tag in parsed_html.find_all(\"td\",{\"class\": \"tabev1\"})]\n",
    "latitude_longitude1\n",
    "latitude_longitude2 = [str(tag.string) for tag in parsed_html.find_all(\"td\",{\"class\": \"tabev2\"})]\n",
    "latitude_longitude2 = [i for i in latitude_longitude2 if \"S\" in i or \"N\" in i or \"E\" in i or \"W\" in i]\n",
    "latitude_longitude = [j.replace(\"\\xa0\",\"\") + \" \" + k.replace(\"\\xa0\",\"\") for j,k in zip(latitude_longitude1,latitude_longitude2)]\n",
    "\n",
    "latitude = [i for i in latitude_longitude if \"S\" in i or \"N\" in i]\n",
    "longitude = [i for i in latitude_longitude if \"E\" in i or \"W\" in i]\n",
    "\n",
    "# Region\n",
    "\n",
    "region = [str(tag.string).replace(\"\\xa0\",\"\") for tag in parsed_html.find_all(\"td\",{\"class\": \"tb_region\"})]\n",
    "\n",
    "# DataFrame\n",
    "\n",
    "earthquakes_dict = {\"Date\":date,\"Time\":time,\"Latitude\":latitude,\"Longitude\":longitude,\"Region\":region}\n",
    "pd.DataFrame(earthquakes_dict).head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"lxml\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English\n",
      "6 629 000+ articles\n",
      "\n",
      "\n",
      "Русский\n",
      "1 900 000+ статей\n",
      "\n",
      "\n",
      "Español\n",
      "1 846 000+ artículos\n",
      "\n",
      "\n",
      "日本語\n",
      "1 366 000+ 記事\n",
      "\n",
      "\n",
      "Deutsch\n",
      "2 781 000+ Artikel\n",
      "\n",
      "\n",
      "Français\n",
      "2 504 000+ articles\n",
      "\n",
      "\n",
      "Italiano\n",
      "1 801 000+ voci\n",
      "\n",
      "\n",
      "中文\n",
      "1 340 000+ 条目 / 條目\n",
      "\n",
      "\n",
      "Português\n",
      "1 102 000+ artigos\n",
      "\n",
      "\n",
      "العربية\n",
      "1 202 000+ مقالة\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "nameList = parsed_html.findAll('a', {'class' : 'link-box'})\n",
    "nameList\n",
    "\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"lxml\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Available Datasets</th>\n",
       "      <th>Specifics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and economy</td>\n",
       "      <td>Small businesses, industry, imports, exports a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime and justice</td>\n",
       "      <td>Courts, police, prison, offenders, borders and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defence</td>\n",
       "      <td>Armed forces, health and safety, search and re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>Students, training, qualifications and the Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Environment</td>\n",
       "      <td>Weather, flooding, rivers, air quality, geolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Government</td>\n",
       "      <td>Staff numbers and pay, local councillors and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Government spending</td>\n",
       "      <td>Includes all payments by government department...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Health</td>\n",
       "      <td>Includes smoking, drugs, alcohol, medicine per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mapping</td>\n",
       "      <td>Addresses, boundaries, land ownership, aerial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Society</td>\n",
       "      <td>Employment, benefits, household finances, pove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Towns and cities</td>\n",
       "      <td>Includes housing, urban planning, leisure, was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Transport</td>\n",
       "      <td>Airports, roads, freight, electric vehicles, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Digital service performance</td>\n",
       "      <td>Cost, usage, completion rate, digital take-up,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Government reference data</td>\n",
       "      <td>Trusted data that is referenced and shared acr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Available Datasets  \\\n",
       "0          Business and economy   \n",
       "1             Crime and justice   \n",
       "2                       Defence   \n",
       "3                     Education   \n",
       "4                   Environment   \n",
       "5                    Government   \n",
       "6           Government spending   \n",
       "7                        Health   \n",
       "8                       Mapping   \n",
       "9                       Society   \n",
       "10             Towns and cities   \n",
       "11                    Transport   \n",
       "12  Digital service performance   \n",
       "13    Government reference data   \n",
       "\n",
       "                                            Specifics  \n",
       "0   Small businesses, industry, imports, exports a...  \n",
       "1   Courts, police, prison, offenders, borders and...  \n",
       "2   Armed forces, health and safety, search and re...  \n",
       "3   Students, training, qualifications and the Nat...  \n",
       "4   Weather, flooding, rivers, air quality, geolog...  \n",
       "5   Staff numbers and pay, local councillors and d...  \n",
       "6   Includes all payments by government department...  \n",
       "7   Includes smoking, drugs, alcohol, medicine per...  \n",
       "8   Addresses, boundaries, land ownership, aerial ...  \n",
       "9   Employment, benefits, household finances, pove...  \n",
       "10  Includes housing, urban planning, leisure, was...  \n",
       "11  Airports, roads, freight, electric vehicles, p...  \n",
       "12  Cost, usage, completion rate, digital take-up,...  \n",
       "13  Trusted data that is referenced and shared acr...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "db_types = [tag.string for tag in parsed_html.find_all(\"a\",{\"class\": \"govuk-link\"})]\n",
    "db_types = db_types[4:]\n",
    "\n",
    "db_sub_types = [tag.string for tag in parsed_html.find_all(\"p\",{\"class\": \"govuk-body\"})]\n",
    "db_sub_types = db_sub_types[1:]\n",
    "\n",
    "datasets_uk = {\"Available Datasets\":db_types,\"Specifics\":db_sub_types}\n",
    "pd.DataFrame(datasets_uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"lxml\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mandarin Chinese',\n",
       " 'Spanish',\n",
       " 'English',\n",
       " 'Hindi',\n",
       " 'Portuguese',\n",
       " 'Bengali',\n",
       " 'Russian',\n",
       " 'Japanese',\n",
       " 'Yue Chinese',\n",
       " 'Vietnamese']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "languages_attrs = [tag.attrs for tag in parsed_html.find_all(\"a\",{\"class\": \"mw-redirect\"})]\n",
    "languages_strings = [tag.string for tag in parsed_html.find_all(\"a\",{\"class\": \"mw-redirect\"})]\n",
    "\n",
    "[i for i,j in zip(languages_strings,languages_attrs) if \"ISO\" in j[\"title\"]][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"lxml\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Initial Release</th>\n",
       "      <th>Director Name</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El padrino (parte II)</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Criadas y señoras</td>\n",
       "      <td>2011</td>\n",
       "      <td>Tate Taylor</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Dersu Uzala (El cazador)</td>\n",
       "      <td>1975</td>\n",
       "      <td>Akira Kurosawa</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Aladdín</td>\n",
       "      <td>1992</td>\n",
       "      <td>Ron Clements</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Bailando con lobos</td>\n",
       "      <td>1990</td>\n",
       "      <td>Kevin Costner</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Gandhi</td>\n",
       "      <td>1982</td>\n",
       "      <td>Richard Attenborough</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Movie Name Initial Release         Director Name Stars\n",
       "0             Cadena perpetua            1994        Frank Darabont   9.2\n",
       "1                  El padrino            1972  Francis Ford Coppola   9.2\n",
       "2         El caballero oscuro            2008     Christopher Nolan   9.0\n",
       "3       El padrino (parte II)            1974  Francis Ford Coppola   9.0\n",
       "4       12 hombres sin piedad            1957          Sidney Lumet   9.0\n",
       "..                        ...             ...                   ...   ...\n",
       "245         Criadas y señoras            2011           Tate Taylor   8.0\n",
       "246  Dersu Uzala (El cazador)            1975        Akira Kurosawa   8.0\n",
       "247                   Aladdín            1992          Ron Clements   8.0\n",
       "248        Bailando con lobos            1990         Kevin Costner   8.0\n",
       "249                    Gandhi            1982  Richard Attenborough   8.0\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "movie_names = [str(tag.string) for tag in parsed_html.find_all(\"a\",{\"title\": True})]\n",
    "movie_names = movie_names[0:250]\n",
    "\n",
    "initial_release = [str(tag.string).replace(\"(\",\"\").replace(\")\",\"\") for tag in parsed_html.find_all(\"span\",{\"class\": \"secondaryInfo\"})]\n",
    "\n",
    "director_name = [tag[\"title\"] for tag in parsed_html.find_all(\"a\",{\"title\": True})]\n",
    "director_name = director_name[0:250]\n",
    "director_name = [d.replace(\"(dir.)\",\"\").strip() for j in [i.split(\",\") for i in director_name] for d in j if \"(dir.)\" in d]\n",
    "\n",
    "ratings = [str(tag.string) for tag in parsed_html.find_all(\"strong\",{\"title\": True})]\n",
    "\n",
    "# DataFrame\n",
    "\n",
    "top250 = {\"Movie Name\":movie_names,\"Initial Release\":initial_release,\"Director Name\":director_name,\"Stars\":ratings}\n",
    "df_top250 = pd.DataFrame(top250)\n",
    "df_top250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hotel Rwanda</td>\n",
       "      <td>2004</td>\n",
       "      <td>Paul Rusesabagina, a hotel manager, houses ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Grapes of Wrath</td>\n",
       "      <td>1940</td>\n",
       "      <td>An Oklahoma family, driven off their farm by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bacheha-Ye aseman</td>\n",
       "      <td>1997</td>\n",
       "      <td>After a boy loses his sister&amp;apos;s pair of sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jaws</td>\n",
       "      <td>1975</td>\n",
       "      <td>When a killer shark unleashes chaos on a beach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>1994</td>\n",
       "      <td>Lion prince Simba and his father are targeted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Raiders of the Lost Ark</td>\n",
       "      <td>1981</td>\n",
       "      <td>In 1936, archaeologist and adventurer Indiana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Persona</td>\n",
       "      <td>1966</td>\n",
       "      <td>A nurse is put in charge of a mute actress and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Apartment</td>\n",
       "      <td>1960</td>\n",
       "      <td>A Manhattan insurance clerk tries to rise in h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Gold Rush</td>\n",
       "      <td>1925</td>\n",
       "      <td>A prospector goes to the Klondike during the 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Movie Name  Year  \\\n",
       "0                                   Hotel Rwanda  2004   \n",
       "1                            The Grapes of Wrath  1940   \n",
       "2                              Bacheha-Ye aseman  1997   \n",
       "3                                           Jaws  1975   \n",
       "4                                  The Lion King  1994   \n",
       "5                        Raiders of the Lost Ark  1981   \n",
       "6                                        Persona  1966   \n",
       "7                                  The Apartment  1960   \n",
       "8  The Lord of the Rings: The Return of the King  2003   \n",
       "9                                  The Gold Rush  1925   \n",
       "\n",
       "                                         Description  \n",
       "0  Paul Rusesabagina, a hotel manager, houses ove...  \n",
       "1  An Oklahoma family, driven off their farm by t...  \n",
       "2  After a boy loses his sister&apos;s pair of sh...  \n",
       "3  When a killer shark unleashes chaos on a beach...  \n",
       "4  Lion prince Simba and his father are targeted ...  \n",
       "5  In 1936, archaeologist and adventurer Indiana ...  \n",
       "6  A nurse is put in charge of a mute actress and...  \n",
       "7  A Manhattan insurance clerk tries to rise in h...  \n",
       "8  Gandalf and Aragorn lead the World of Men agai...  \n",
       "9  A prospector goes to the Klondike during the 1...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "movie_urls = [\"https://www.imdb.com/\"+str(tag[\"href\"]) for tag in parsed_html.find_all(\"a\",{\"href\": re.compile(\"pf_rd_m\")})]\n",
    "movie_urls = [i for i in movie_urls if \"chttp_tt_\" in i]\n",
    "movie_urls = movie_urls[0::2]\n",
    "movie_urls\n",
    "\n",
    "df_top250[\"Detail URL\"] = movie_urls\n",
    "df_top250_sample = df_top250.sample(n = 10)\n",
    "\n",
    "movies_html = []\n",
    "\n",
    "for url in df_top250_sample[\"Detail URL\"]:\n",
    "    head = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "    resp = requests.get(url,\n",
    "                        headers=head,\n",
    "                        timeout=5)\n",
    "    ht = resp.content\n",
    "    movies_html.append(bs4.BeautifulSoup(ht, \"lxml\"))\n",
    "\n",
    "\n",
    "movie_data = [json.loads(tag.string) for data in movies_html for tag in data.find_all(\"script\",{\"type\": \"application/ld+json\"})]\n",
    "\n",
    "movie_name = [movie_data[i][\"name\"] for i in range(len(movie_data))]\n",
    "movie_description = [movie_data[i][\"description\"] for i in range(len(movie_data))]\n",
    "\n",
    "top10 = {\"Movie Name\":movie_name,\n",
    "         \"Description\":movie_description}\n",
    "\n",
    "df_top10 = pd.DataFrame(top10, index=[i for i in df_top250_sample.index])\n",
    "df_top10 = pd.DataFrame({\"Movie Name\": list(df_top10[\"Movie Name\"].values),\n",
    "                         \"Year\": list(df_top250_sample[\"Initial Release\"].values),\n",
    "                         \"Description\": list(df_top10[\"Description\"].values)})\n",
    "df_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "response = requests.get(url,\n",
    "                        headers=headers,\n",
    "                        timeout=5)\n",
    "\n",
    "html = response.content\n",
    "parsed_html = bs4.BeautifulSoup(html, \"html.parser\") \n",
    "#print(parsed_html.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Book Name   Price Stock Availability\n",
       "0                      A Light in the ...  £51.77           In stock\n",
       "1                      Tipping the Velvet  £53.74           In stock\n",
       "2                              Soumission  £50.10           In stock\n",
       "3                           Sharp Objects  £47.82           In stock\n",
       "4            Sapiens: A Brief History ...  £54.23           In stock\n",
       "5                         The Requiem Red  £22.65           In stock\n",
       "6            The Dirty Little Secrets ...  £33.34           In stock\n",
       "7                 The Coming Woman: A ...  £17.93           In stock\n",
       "8                     The Boys in the ...  £22.60           In stock\n",
       "9                         The Black Maria  £52.15           In stock\n",
       "10  Starving Hearts (Triangular Trade ...  £13.99           In stock\n",
       "11                  Shakespeare's Sonnets  £20.66           In stock\n",
       "12                            Set Me Free  £17.46           In stock\n",
       "13    Scott Pilgrim's Precious Little ...  £52.29           In stock\n",
       "14                      Rip it Up and ...  £35.02           In stock\n",
       "15                  Our Band Could Be ...  £57.25           In stock\n",
       "16                                   Olio  £23.88           In stock\n",
       "17        Mesaerion: The Best Science ...  £37.59           In stock\n",
       "18           Libertarianism for Beginners  £51.33           In stock\n",
       "19                It's Only the Himalayas  £45.17           In stock"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "book_name = [tag.string for tag in parsed_html.find_all(\"a\",{\"title\": True})]\n",
    "price = [tag.string for tag in parsed_html.find_all(\"p\",{\"class\": \"price_color\"})]\n",
    "stock_availability = [tag.text.strip() for tag in parsed_html.find_all('p', {'class': 'instock availability'})]\n",
    "\n",
    "df_books = pd.DataFrame({\"Book Name\": book_name,\n",
    "                         \"Price\": price,\n",
    "                         \"Stock Availability\": stock_availability})\n",
    "df_books"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m1_env]",
   "language": "python",
   "name": "conda-env-m1_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
